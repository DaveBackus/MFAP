\documentclass[11pt]{article}

\input{../LaTeX/preamble.tex}

\begin{document}
\parskip=\bigskipamount
\parindent=0.0in
\thispagestyle{empty}
\input{../LaTeX/header.tex}

\bigskip\bigskip
\centerline{\Large \bf Math Tools:  Random Variables}
\centerline{Revised: \today}

\medskip
Our first set of math tools is concerned with randomness:
how we think about it, how we use it, and so on.
It's an essential idea in macroeconomics and finance ---
in fact in the world at large --- and the tools developed
to work with it are incredibly useful.
Remind yourself as we work through this material:
Tools make life easier,
even if it takes some time to learn how to use them.
The plan is to describe some concepts and tools and put them to work.
We won't worry too much about mathematical niceties, but
you can find more formal versions elsewhere.

We'll use three concepts to describe randomness:  random variables, distributions, and moments.
I know this sounds a little circular, but {\it random variables\/} describe things that are random.
They are characterized by their {\it distributions\/},
the probabilities of possible outcomes.
{\it Moments\/} are summary measures that tell us something about
the distribution.

Moments are, in some ways, crude measures of distributions,
but we use them to describe two sets of properties.
The mean and variance reflect a random variable's
location (where it is on the real line)
and dispersion (how spread out it is).
If the world is normal, then that's all we need.
But of course lots of things in this world aren't normal.
Skewness and kurtosis reflect the shape of the distribution
and helpful to identify deviations from the normal distribution.
They will show up when we look at risk premiums, option prices,
and other things.


\section{Random variables and probability distributions}

Random variables are used to describe things that are random,
typically in the sense that we don't know their outcomes ahead of time.
You might think of the weather (rain or shine?),
the economy (boom, bust, or somewhere in between?),
the stock market (ditto),
or sports (will the Steelers win?).

To make this precise, it's helpful to build the definition
from components.
This will start off abstract, but turn concrete quickly --- and stay that way.
The components are:
%
\begin{itemize}
\item {\it States.\/}  Let's start with what we'll call a {\it state\/},
one of the possible outcomes of a random process.
You may see the terms {\it outcome\/} or {\it event} used instead,
but state serves our purposes.
We represent a state with the letter $z$
and the set of all possible states --- the {\it state space\/} --- by $\mathcal{Z}$.
[Draw event tree here.]
Sometimes state $z=1$ occurs, sometimes state $z=2$, and so on.
Part of the art of applied work is to come up with
a useful practical definition of $\mathcal{Z}$.
If we're talking about the weather, the states might be
$z = \mbox{rain}$ and $z = \mbox{shine}$.
If we're talking about the stock market, we might assign different states to every possible value
of the S\&P 500 on (say) the last day of the year.

\item {\it Probabilities.\/}
For each state $z$, we assign a number $\mbox{Prob}(z) = p(z)$
that we refer to as the {\it probability\/} of $z$.
Here $\mbox{Prob}(A)$ means (literally) the probability of
some arbitrary subset $A$ of $\mathcal{Z}$
and $p$ is a function of the state $z$.
We call the complete collection of $p(z)$'s
the {\it probability distribution\/}.
Not every set of numbers works.
Legitimate probabilities must be nonnegative and sum to one.

\item {\it Random variables.\/}
A {\it random variable\/} is a function that
assigns a real number to every state:  $x(z)$.
Note that $x$ inherits randomness from $z$ so it is, precisely,
a random variable.
Sometimes people distinguish between the random variable
and the values the random variable takes,
but we'll use $x$ for both.
\end{itemize}
%
In practice we often ignore $z$
and define probabilities directly over $x$,
but we'll see later on that there are times when the distinction
between states and random variables is helpful.

Some common examples of probability distributions of random variables:
%
\begin{itemize}
\item {\it Bernoulli.\/}
The state space $\mathcal{Z}$ has two elements:  $\mathcal{Z} = \{z_1, z_2\}$.
If we're flipping a coin, $z_1$ might represent heads and $z_2$ tails.
A random variable assigns numbers to these two states.
The simplest version uses zero and one:
$x(z_1) = 0$, $x(z_2) = 1$.
The probabilities $p(z_1)$ and $p(z_2)$ are nonnegative numbers that sum to one.


\item {\it Poisson.\/}
This is a little more complicated, but it's a beautiful example
that's often used in finance (many other places, too, of course).
Suppose $\mathcal{Z}$ consists of the nonnegative integers:  $\{ 0, 1, 2, \ldots \}$.
The probability of any particular $z$ is
\begin{eqnarray*}
    p(z) &=& e^{-\omega} \omega^z/z! ,
\end{eqnarray*}
with parameter $\omega > 0$ (``intensity'').


Are they legitimate probabilities?
Well, they're all positive, so we're good there.
Do they sum to one?
That's more complicated.
The exponential function has the power series expansion
\begin{eqnarray}
    e^x &=& 1 + x + x^2/2 + x^3/3! + x^4/4! + \cdots
            \;\;=\;\; \sum_{j=0}^\infty x^j/j!.
    \label{eq:eofx-powerseries}
\end{eqnarray}
(This is the Taylor series representation of $e^x$ at the point $x=0$.)
Our probabilities have  a similar form:
\begin{eqnarray*}
    \sum_{z=0}^\infty p(z) &=& e^{-\omega} \sum_{z=0}^\infty \omega^z/z!
                \;\;=\;\; e^{-\omega} e^{\omega}
                \;\;=\;\; 1.
\end{eqnarray*}
So they are, in fact, legitimate probabilities. % for the state space $\mathcal{Z}$.

\item {\it Normal (Gaussian).\/}
Here we'll let the state space be the real line.
We'll also set $x=z$, which allows us to ignore $z$.
We refer to such random variables as continuous,
to distinguish them from random variables that
based on a discrete set of states, such as the two we just looked at.

For continuous random variables,
we describe probabilities
with what's called a {\it probability density function\/} or pdf $p(x)$.
Probabilities over an interval $[a,b]$ are integrals,
\begin{eqnarray*}
    \mbox{Prob} ( a\leq x \leq b ) &=& \int_{a}^b p(x) dx .
\end{eqnarray*}
The function $p(x)$ must be nonnegative for all values of $x$
and integrate to one,
\begin{eqnarray*}
    \int_{-\infty}^\infty p(x) dx &=& 1 ,
\end{eqnarray*}
the analog of the sum we used earlier.
The counterpart of $p(x)$ in the discrete case is $p(x) dx$
--- it's important to include the $dx$.
%(The integral sign is, in fact, a stylized S for sum.)

A {\it normal\/} or Gaussian random variable has density
function
\begin{eqnarray*}
    p(x) &=& (2 \pi \sigma^2)^{-1/2} \exp[-(x-\mu)^2/(2\sigma^2)] ,
\end{eqnarray*}
the well-known ``bell-shaped curve.''
[If you graph this, you'll see why.]
It's positive for all $x$ and integrates to one,
although (for good reason) we'll take the latter on faith rather than demonstrate it.
%
The two parameters are $\mu$ and $\sigma^2$ so we often
write $ x \sim \mathcal{N}(\mu, \sigma^2)$ to mean
``$x$ is normal with parameters $\mu$ and $\sigma^2$.''
We'll see shortly what these parameters do.
%
%If you plot it, you'll see that $p$ is symmetric around $x=\mu$,
%which I guess is obvious from the formula.
The so-called {\it standard normal\/} refers to the case
$\mu = 0$ and $\sigma = 1$.
[Graph $p(x)$ and show how it changes when we change $\mu$ and $\sigma$.]
\end{itemize}

There are lots of other common distributions of random variables,
many of which are summarized in Wikipedia:
search ``normal distribution'' or ``Bernoulli distribution.''
(I know Wikipedia
has a bad rep, but the math content is pretty good.)


\section{Expectations and moments}

The behavior of a random variable is described completely
by (i)~the set of values it can take
and (ii)~their probabilities.
Sometimes that's too much information:  we'd like a smaller number
of properties that capture some salient features.
I like looking at the whole distribution,
or even better a picture of it,
but summary numbers can be useful, too.

We start with the concept of an {\it expectation\/}.
If $x$ is a random variable with probabilities $p$,
its expectation is the weighted average value of $x$
using probabilities as weights:
\begin{eqnarray*}
    E (x) &=& \sum_z x(z) p[x(z)]  .
\end{eqnarray*}
For a continuous random variable,
%one for which the state space $Z$ is a part of the real line --- % ??
we replace the sum with an integral.
The notation is a little cumbersome, which is why
we just write $E(x)$ most of the time.

We can extend this concept to any function of $x$:
\begin{eqnarray}
    E [f(x)] &=& \sum_z  f[x(z)] p[x(z)] .
    \label{eq:expectation}
\end{eqnarray}
There may be cases where the sum (or integral) doesn't converge,
but we won't worry about that now --- or ever, really.

We see from the definition (\ref{eq:expectation}) that expectations are linear,
which means they satisfy these two properties.
(i)~If $f$ is a sum, so that $f(x) = g_1(x) + g_1(x)$ for two functions $g_1$ and $g_2$,
then its expectation is the sum of the expectations of its components.
In short-hand notation,
\begin{eqnarray*}
    E[f(x)] &=& E[g_1(x)] + E[g_2(x)] .
\end{eqnarray*}
(ii)~If we multiply $f$ by a constant $c$, then its expectation is $c$ times
the expectation of the original function:
\begin{eqnarray*}
    E[cf(x)] &=& c E[f(x)] .
\end{eqnarray*}
Together they imply that the expectation of a linear function of $x$
is the same linear function of its expectation:
\begin{eqnarray*}
    E(a + bx) &=& a + b E(x) .
\end{eqnarray*}
We'll use these properties so often they'll become second nature.


Here's a well-known application.
A {\it moment\/} is the expectation of a power of $x$:
\begin{eqnarray*}
    \mu_j^\prime &=& E (x^j)
\end{eqnarray*}
for $j$ a positive integer.
The first one ($\mu_1^\prime$) is known as the {\it mean\/}.
It's a measure of the ``location'' of the probability distribution.
If we change the mean,  the probability distribution shifts left or right.
Think about graphing the probability distribution
of $x+a$ for different values of $a$.
If the mean of $x$ is $\mu^\prime_1$, then the mean of $x+a$ is
$\mu^\prime_1 + a$,
so when we move the distribution back and forth by changing $a$,
that's reflected in the mean.

We often use {\it central moments\/} instead, meaning we look at powers
of $x$ minus its mean:
\begin{eqnarray*}
    \mu_j &=& E [(x-\mu_1^\prime)^j]
\end{eqnarray*}
The idea is to take location out of the calculation.
The first central moment is zero by construction:
\begin{eqnarray*}
    \mu_1 &=& E (x-\mu_1^\prime)
            \;\;=\;\;  E(x) - \mu_1^\prime
            \;\;=\;\;  \mu_1^\prime - \mu_1^\prime  \;\;=\;\; 0.
\end{eqnarray*}
(Here and elsewhere: If any of these steps seem mysterious,
write out the calculation of the expectation for the discrete case.)


The second central moment is called the {\it variance\/}.
It's a measure of dispersion:  how spread out or dispersed the distribution is.
We denote it by
\begin{eqnarray*}
    \mbox{Var}(x)  &=&  E [(x-\mu_1^\prime)^2] .
\end{eqnarray*}
Since squares are positive (or at least nonnegative), so is the variance.
If we write out the definition of the variance,
we see it can be expressed in terms of (noncentral or raw) moments:
\begin{eqnarray*}
    E [(x-\mu_1^\prime)^2]  &=&  E [x^2 - 2 x\mu_1^\prime + (\mu_1^\prime)^2]
            \;\;=\;\; \mu_2^\prime - 2 (\mu_1^\prime)^2 + (\mu_1^\prime)^2
            \;\;=\;\; \mu_2^\prime -  (\mu_1^\prime)^2 .
\end{eqnarray*}
If we wanted to, we could compute the variance this way.
That's pretty common, but we'll see shortly there's a
better (by which I mean easier) way.
The {\it standard deviation\/} is the (positive) square root of the variance
and is often used the same way.

Let's see how the mean and standard deviation reflect
location and scale, respectively.
Suppose $x$ has given mean and standard deviation.
What are the mean and standard deviation of $y = a + b x$?
The mean is $ E(y) = a + b E(x)$,
so $a$ moves it up and down.
The variance is
\begin{eqnarray*}
    \mbox{Var}(x) &=& E \left\{ [y - E(y)]^2 \right\}
        \;\;=\;\; E \left\{ [a + b x - a - b E(x)]^2 \right\} \\
        &=& E \left\{  [ b x - b E(x)]^2 \right\}
        \;\;=\;\; b^2 E \left\{ [ x - E(x)]^2 \right\}
        \;\;=\;\; b^2 \mbox{Var}(x) .
\end{eqnarray*}
The standard deviation is the positive square root.
If $\sigma$ is the standard deviation of $x$,
then $|b| \sigma$ is the standard deviation of $y$.
(Why absolute value?  The standard deviation is the positive square root of the variance.)
Note that all of this follows from applying definitions.

%It measures the ``scale'' of a random variable
%in the following sense:
%If the standard deviation of $x$ is $\sigma>0$,
%then the standard deviation of $ax$ is $| a | \sigma$.
%Why absolute value?  The variance of $ax$ is $a^2 \sigma^2$.
%The standard deviation is the positive square root.


Let's go back to our examples and see what the mean and variance are:
%
\begin{itemize}
\item {\it Bernoulli.\/}
Let the probability that $x=1$ be $\omega$ and the probability that $x=0$ be $1-\omega$.
How do we find the mean and variance?
The easiest way is to look them up in Wikipedia (search ``Bernoulli distribution''),
but let's see if we can find them on our own.
The mean is (apply the definition)
\begin{eqnarray*}
    E(x) &=&  (1-\omega) \cdot 0 + \omega \cdot 1
            \;\;=\;\; \omega.
\end{eqnarray*}
We find the variance from the second moment:
\begin{eqnarray*}
    E(x^2) &=&  (1-\omega) \cdot 0^2 + \omega \cdot 1^2
            \;\;=\;\; \omega.
\end{eqnarray*}
The variance is therefore
\begin{eqnarray*}
    \mbox{Var}(x) &=& E(x^2) - [E(x)]^2
            \;\;=\;\;  \omega - \omega^2
            \;\;=\;\; \omega (1-\omega) .
\end{eqnarray*}
The standard deviation is the (positive) square root of this.
%You might ask:  for what value of $\omega$ is the variance highest?
%The answer:  $\omega=1/2$.


\item {\it Poisson.\/}
The mean is
\begin{eqnarray*}
    E(x) &=& e^{-\omega} \sum_{j=0}^\infty j \omega^j/j!
            \;\;=\;\; e^{-\omega} \omega \sum_{j=1}^\infty (j-1) \omega^{j-1}/(j-1)!
            \;\;=\;\; \omega .
\end{eqnarray*}
[You'll have to think about this a little -- or better yet, wait a few minutes and
we'll derive this by an easier route.]
The second moment is
\begin{eqnarray*}
    E(x^2) &=& e^{-\omega} \sum_{j=0}^\infty j^2 \omega^j/j!
            \;\;=\;\;  ??.
\end{eqnarray*}
We could fight our way through this,
but since an easier way is just around the corner,
I'll surrender now and come back to fight another day.


\item {\it Normal.\/}
We'll postpone this one, too.
\end{itemize}

Our last topic here is {\it sample moments\/}:
moments computed from data.
The idea is to use sample weights rather than probabilities.
Given a sample of $x_t$'s for $t=1,2,\ldots,T$,
the sample mean is
\begin{eqnarray*}
         \bar{x} &=& T^{-1} \sum_{t=1}^T x_t .
\end{eqnarray*}
Similarly, the $j$th sample moment is
\begin{eqnarray*}
         T^{-1} \sum_{t=1}^T x_t^j .
\end{eqnarray*}
The $j$th sample {\it central\/} moment is
\begin{eqnarray*}
         T^{-1} \sum_{t=1}^T (x_t-\bar{x})^j .
\end{eqnarray*}
If $j=2$ we get the sample variance.
And if we take the square root, we get the sample standard deviation.
(And yes, we divide by $T$, not $T-1$ or something else.)

We use sample moments the same way we use moments:  to describe the distribution.
Summary statistics include the mean, the variance, the standard deviation,
and so on.
If the $x_t$'s are produced by a specific distribution,
then with enough data we would hope that the sample moments will be ``close''
to the moments of the distribution that generated them.
%That's worth showing,
%but we need to leave a few things for other courses.


\section{Generating functions}

Next up is one of my favorite tools:  generating functions.
It's a tool with a wide range of uses,
but we're interested in one:
as a short-cut in computing moments.
If you've run across Laplace, {Fourier}, or $z$ transforms,
they're closely related.

The {\it moment generating function\/} (mgf) is defined by
\begin{eqnarray}
    h(s) &=& E \left( e^{s x} \right) ,
    \label{eq:def-mgf}
\end{eqnarray}
a function of the real number $s$.
(It's common to use $t$ instead of $s$, but we need $t$ for time.)
Note that $h(0) = 1$.
[Ask yourself why.]
% ?? [You might also run across references to the characteristic function
%$ h(is) = E ( e^{is x} ) $, where $i$ is the square root of $-1$. Ignore them.]

The mgf is a tool, like a hammer, and we hammer things with it.
If we hammer probability distributions, we get moments as a byproduct.
Recall the Taylor series expansion (\ref{eq:eofx-powerseries}) of the exponential function.
If we expand $e^{sx}$ the same way and take expectations,
the moments pop out:
\begin{eqnarray*}
    h(s) &=& E \left[ 1 + (sx) + (sx)^2/2 + (sx)^3/3! + \cdots \right] \\
            &=&  1 + \mu_1^\prime s + \mu_3^\prime (s^2/2)  + \mu_3^\prime (s^3/3!)  + \cdots .
\end{eqnarray*}
With a little more insight, we see that we can recover the moments by differentiating
$h$ and setting $s=0$.
The first derivative is the mean:
\begin{eqnarray*}
    h^{(1)} (0)  &=& \left. \frac{ d h(s)}{d s} \right|_{s=0}
            \;\;=\;\;  \mu_1^\prime .
\end{eqnarray*}
Here $h^{(1)}(0)$ means the first derivative of the function $h(s)$ evaluated at
$s=0$.
Similarly, high-order moments follow from high-order derivatives:
\begin{eqnarray*}
    h^{(j)} (s)  &=& \left. \frac{d^j h(s)}{d s^j} \right|_{s=0}
            \;\;=\;\;  \mu_j^\prime
\end{eqnarray*}
This looks horrible, but it just says that the $j$th moment is the $j$th derivative
evaluated at $s=0$.
Bottom line:  if we know the mgf, we can find moments by differentiating it.
Better yet, we can get Matlab to do the differentiating.

Let's go back to our examples:
%
\begin{itemize}
\item {\it Bernoulli.\/}
The mgf is
\begin{eqnarray*}
    h(s) &=& (1-\omega) e^{s \cdot 0} + \omega e^{s \cdot 1}
        \;\;=\;\; (1-\omega) + \omega e^{s} .
\end{eqnarray*}
The first two derivatives give us the first two (noncentral) moments:
\begin{eqnarray*}
    h^{(1)}(0) &=& \omega \;\;=\;\; \mu_1^\prime \\
    h^{(2)}(0) &=& \omega \;\;=\;\; \mu_2^\prime.
\end{eqnarray*}
The variance is therefore $\mu_2^\prime - (\mu_1^\prime)^2 = \omega (1-\omega)$, as we saw earlier.

\item {\it Poisson.\/}  The mgf is
\begin{eqnarray*}
    h(s) &=& \sum_{z=0}^\infty e^{sz} e^{-\omega} \omega^z/z!
            \;\;=\;\; e^{-\omega} \sum_{z=0}^\infty \left( e^{s} \omega\right)^z /z!
            \;\;=\;\; e^{-\omega} e^{e^{s} \omega}
            \;\;=\;\; e^{\omega (e^{s}-1)}  .
\end{eqnarray*}
The first two derivatives are
\begin{eqnarray*}
    h^{(1)}(0) &=& \omega  \\
    h^{(2)}(0) &=& \omega + \omega^2 .
\end{eqnarray*}
The mean and variance are therefore both equal to $\omega$.

\item {\it Normal.\/}
We find the mgf by completing the square.
Pay attention here:  we'll see this over and over again, including our derivation
of the Black-Scholes-Merton option pricing formula.
We start with the definition:
\begin{eqnarray*}
        h(s) &=& (2\pi \sigma^2)^{-1/2} \int_{-\infty}^\infty e^{sx} e^{-(x-\mu)^2/2\sigma^2} dx .
\end{eqnarray*}
The exponents are

% see http://www.physicsforums.com/showthread.php?t=276669
\smallskip
\newlength{\oldfrule}
\setlength{\oldfrule}{\fboxrule}
\setlength{\fboxrule}{3\fboxrule}
\newlength{\boxwidth}
\setlength{\boxwidth}{\textwidth}
\addtolength{\boxwidth}{-2\fboxsep}
\addtolength{\boxwidth}{-2\fboxrule}
\noindent
\fbox{%
\begin{minipage}{\boxwidth}
\vspace{-0.75\abovedisplayskip}
%\vspace{0.0in}
\begin{eqnarray}
        sx - (x-\mu)^2/2\sigma^2 &=&
                -(1/2\sigma^2) \left[ -2 \sigma^2 s x + x^2 - 2 \mu x + \mu^2  \right]
                        \nonumber \\
            &=& \mu s + \sigma^2 s^2/2 - [x-(\mu+s \sigma^2)]^2/2\sigma^2 .
            \label{eq:complete-the-square}
\end{eqnarray}
\vspace{-1.25\abovedisplayskip}
\end{minipage}}
\setlength{\fboxrule}{\oldfrule}

This may take you a couple minutes,  but try expanding both expressions and lining up terms.
I put a box around it, because it's an equation we'll see again,
specifically when we come to option prices.

When we plug the result into the integral and rearrange terms,
we have
\begin{eqnarray*}
        h(s) &=& e^{\mu s + \sigma^2 s^2/2} \; (2\pi \sigma^2)^{-1/2}
        \int_{-\infty}^\infty e^{-[x-(\mu+s\sigma^2)]^2/2\sigma^2} dx .
\end{eqnarray*}
The last term is a normal density function
and therefore integrates to one.
[We didn't prove this, but we know pdf's must integrate to one.  Right?]
That leaves us with
\begin{eqnarray*}
        h(s) &=& e^{\mu s + \sigma^2 s^2/2}  .
\end{eqnarray*}
If you differentiate, you can show that this implies
a mean of $\mu$ and a variance of $\sigma^2$.
%[You might try this yourself, to make sure you're following the logic.]
\end{itemize}


The moment generating function gives us, in these and many other cases,
an easy route to finding moments.
The {\it cumulant generating function\/} (cgf) is even better.
It's defined as the logarithm of the moment generating function:
\begin{eqnarray}
    k(s) &=& \log h(s) .
    \label{eq:def-cgf}
\end{eqnarray}
Note well:  In this class, and in Matlab,
$\log$ means the natural or base-$e$ logarithm.
Always.

The cgf has the Taylor series expansion
\begin{eqnarray}
    k(s) &=& \kappa_1 s + \kappa_2 s^2/2 + \kappa_3 s^3/3! + \cdots  ,
    \label{eq:cumulant-expansion}
\end{eqnarray}
where $\kappa_j$ is the $j$th derivative of $k(s)$ at $s=0$:
\begin{eqnarray*}
    \kappa_j &=& k^{(j)}(0) .
\end{eqnarray*}
[Question:  Why is there no $k(0)$ term?]
We refer to these derivatives as {\it cumulants\/} $\kappa_j$.

The question is what the cumulants are, other than derivatives of the cgf.
With the moment generating function, we could see that the coefficients
were raw moments.
But what are they here?
The best way to answer that is to connect the cumulants to moments,
which we do by linking derivatives of $k$
to those of $h$ using (\ref{eq:def-cgf}).
For example, the first two derivatives are
\begin{eqnarray*}
    k^{(1)}(0) &=& h^{(1)}(0) / h(0) \;\;=\;\; h^{(1)}(0) \\
    k^{(2)}(0) &=& h^{(2)}(0) - h^{(1)}(0)^2 ,
\end{eqnarray*}
the mean and variance.

Note that the second cumulant ``centralizes'' the second moment:
we get the variance directly, rather than the second noncentral moment.
That's an example of a more general property.
The cgf of $y = a+bx$ is
\begin{eqnarray*}
    k(s; y) &=& a s + k(sb; x) .
\end{eqnarray*}
[This may look mysterious, but just apply the definition:
$ h(s; y) = E[\exp(a s + bs x)] = \exp(as) E[\exp(bs x)] = \exp(as) h(bs; y)$ and take logs.]
As we've seen, $a$ changes the location and $b$ changes the scale.
The cumulants of $y$ are therefore connected to those of $x$ by
\begin{eqnarray*}
    \kappa_1 (y) &=& a + b \kappa_1 (x) \\
    \kappa_j (y) &=& b^j \kappa_j (x) \;\;\; \mbox{ for } j=2,3,\ldots
\end{eqnarray*}
That is:  after the first one, cumulants aren't affected by location,
and scale shows up as a power.

After the mean and variance, the most useful moments/cumulants
are the third and fourth measuring, respectively,
{\it skewness\/} and {\it kurtosis\/}.
Skewness refers to the asymmetry of the distribution:
odd cumulants (and central moments) are zero after the first
for any symmetric distribution.
[Can you show this?]
Kurtosis (sort of) refers to how much weight is in the tails of the density;
holding the mean and variance constant, a distribution
with greater kurtosis will have more weight in the tails and,
to keep the variance constant, more near the center as well.
There's no theorem to that effect, but it's a useful statement nonetheless.
[Draw a picture.]

The standard measures of skewness and kurtosis
are based on the third and fourth cumulants:
\begin{eqnarray*}
    \gamma_1 &=& \kappa_3 /(\kappa_2)^{3/2} \;\;\; \mbox{(skewness)}\\
    \gamma_2 &=& \kappa_4 /(\kappa_2)^2 \;\;\;\;\;\; \mbox{(excess kurtosis)}
\end{eqnarray*}
%[Ask yourself:  what are the inputs to these expressions?]
The denominators take care of scaling.
[If you don't see this, note how scaling affects cumulants,
and therefore $\gamma_1$ and $\gamma_2$.]
We'll explain the term ``excess'' shortly.

% ?? Make location and scale a sep section
%  do std normal as example:  x N(0,1), then y = a + b x is N(a,b^2).

\needspace{4\baselineskip}
Our examples again:
%
\begin{itemize}
\item {\it Bernoulli.\/}
The cgf is
\begin{eqnarray*}
    k(s) &=& \log \left[ (1-\omega) + \omega e^{s} \right] .
\end{eqnarray*}
For practice:  compute the first four cumulants and the
measures of skewness and excess kurtosis.

\item {\it Poisson.\/}  The cgf is
\begin{eqnarray*}
    k(s) &=& \omega (e^{s}-1)  .
\end{eqnarray*}
Its derivatives are all the same, so we have
\begin{eqnarray*}
    \kappa_j &=& \omega
\end{eqnarray*}
for all $j\geq 1$.
Skewness is $\gamma_1 = \omega/\omega^{3/2} = \omega^{-1/2} > 0$.
Excess kurtosis is
$\gamma_1 = \omega/\omega^{2} = \omega^{-1} > 0$.


\item {\it Normal.\/}
The cgf is
\begin{eqnarray*}
        k(s) &=& \mu s + \sigma^2 s^2/2  .
\end{eqnarray*}
What's wonderful about this is that all cumulants after the first two are
zero.
Any nonzero cumulants beyond that are signs
that the distribution isn't normal.
Skewness and excess kurtosis are examples of that.

One last thing:  Why do we say ``excess'' kurtosis?
An alternative measure of kurtosis
follows from the fourth central moment:
\begin{eqnarray*}
    \mu_4 /(\mu_2)^2 .
\end{eqnarray*}
Since $\mu_2 = \kappa_2$, only the numerator differs
from our earlier measure.
And since neither measure depends on location or scale,
it's enough to look at the standard normal,
which has $\mu = 0$ and $\sigma = 1$.
The mgf is therefore $h(s) = e^{s^2/2}$ and its fourth derivative is
\begin{eqnarray*}
    h^{(4)}(s) &=& 3 e^{s^2/2} + 6 s^2 e^{s^2/2} + s^4 e^{s^2/2} .
\end{eqnarray*}
[I did this in Matlab because I'm lazy.]
The fourth moment (central because the mean is zero) is
therefore $ h^{(4)}(0) = 3$.
Why 3?  That's just the way it is.
But it tells us that if the kurtosis of a normal random variable is three,
we need to subtract three to detect departures from normality.
That's what $\gamma_2$ does.
\end{itemize}

%\section{Lognormal random variables} ??




\section{Relations between random variables}

So far we've looked at single random variables.
But economics and finance --- and lots of other things as well ---
are concerned not with the properties
of single random variables, but with relations among two or more of them.
It's not enough to know the distributions of GDP growth
and equity returns,
we'd also like to know if they're related, and if so, how.
We need a language for talking about that.
%We'll use the simplest available --- covariances and correlations ---
%

We start with {\it independent\/} random variables,
which we define shortly.
The setup for ``multivariate'' (more than one) random variables
is similar to what we've seen.
The probability density function for a two-dimensional random variable
$(x_1,x_2)$ might be expressed $p(x_1,x_2)$.
We say $x_1$ and $x_2$ are independent if this factors into separate functions:
$p(x_1,x_2) = p_1(x_1) p_2(x_2) $.
You might think of coin flips.
If two flips are independent, then the probability of two heads (say)
is just the product of each head separately.
If the probability of heads is one-half each time,
then the probability of two heads is one-fourth.
That's what independence is.

Dependence  ---
 the opposite of independence ---
comes in many forms.
The most direct connection is a linear one,
which we can document with {\it covariances\/}
and {\it correlations\/}.
The covariance between two  random variables $x_1$ and $x_2$ is
\begin{eqnarray*}
    \mbox{Cov} (x_1,x_2) &=& E [x_{1} - E({x}_1)] [x_{2} - E({x}_2)] .
\end{eqnarray*}
It's an example of a ``joint moment,''
a moment that involves two or more random variables.
If the covariance is positive, high values of $x_1$ are associated,
more often than not, with high values of $x_2$.
If negative, the reverse.
Their correlation is a scale-free version:
\begin{eqnarray}
    \mbox{Corr}(x_1,x_2) &=&
            \frac {\mbox{Cov}(x_1,x_2)} {\mbox{Var}(x_1)^{1/2} \mbox{Var}(x_2)^{1/2} } ,
    \label{eq:correlation}
\end{eqnarray}
a number between minus one and one.
You might verify for yourself that if we look instead at the
variables $a + bx_1$ and $c + dx_2$, the correlation is the same:
$a$ and $c$ drop out when we subtract the means and,
except for sign, $b$ and $d$ cancel when we calculate the ratio.

Sample analogs are more or less predictable.
The sample means and variances, of course, are
\begin{eqnarray*}
    \bar{x}_i &=& T^{-1} \sum_{t=1}^T x_{it}, \;\;\;
    \mbox{Var} (x_i) \;\;=\;\; T^{-1}
            \sum_{t=1}^T (x_{it} - \bar{x}_i)^2
\end{eqnarray*}
for variables $i=1,2$.
The sample covariance of $x_1$ and $x_2$ is
\begin{eqnarray*}
    \mbox{Cov} (x_1,x_2) &=& T^{-1}
            \sum_{t=1}^T (x_{1t} - \bar{x}_1) (x_{2t} - \bar{x}_2)
\end{eqnarray*}
and the sample correlation is constructed from the sample variances and covariance by
(\ref{eq:correlation}).
%
%\begin{eqnarray*}
%    \mbox{Corr}(x_1,x_2) &=&
%            \frac {\mbox{Cov}(x_1,x_2)} {\mbox{Var}(x_1)^{1/2} \mbox{Var}(x_2)^{1/2} } .
%\end{eqnarray*}
You can generally get the idea from a scatterplot of the two variables.
If you're taken by this possibility, search ``Anscombe's quartet.''


Correlation is a measure of linear association.
It's entirely possible for two variables to be related
in a nonlinear way but have a zero covariance and correlation.
Consider this example:
Let $x_1$ take on the values $\{-1,0,1\}$ with probability one-third each.
Then let $x_2 = (x_1)^2$.
The two variables are clearly related, but their covariance is zero.



\section{Sums and mixtures}

We'll sometimes create random variables from combinations, either
sums or mixtures of independent components.
Here's how that works.

{\it Sums.\/}
Suppose we start with the independent random variables $x_1$ and $x_2$.
The distribution of the sum $y = x_1+x_2$
depends (evidently) on the distributions of $x_1$ and $x_2$.
The mgf of $y$ is
\begin{eqnarray*}
    h_y(s) &=& E \left( e^{sy} \right)
            \;\;=\;\; E \left( e^{s(x_1+x_2)} \right)
            \;\;=\;\; E \left( e^{s x_1} e^{s x_2} \right)
            \;\;=\;\; E \left( e^{s x_1}\right) E \left( e^{s x_2} \right) .
\end{eqnarray*}
The last step follows from independence of $x_1$ and $x_2$.
It implies $h_y(s) = h_1(s) h_2(s)$:  the mgf of the sum
is the product of the mgf's of the components.


The cgf's of sums are even simpler.
The cgf of the sum is the sum of the cgf's:
\begin{eqnarray}
    k_y(s) &=& \log h_y(s)
            \;\;=\;\; \log h_1(s) + \log h_2(s)
            \;\;=\;\; k_1(s) + k_2(s) .
            \label{eq:cgf-sum}
\end{eqnarray}
[Think about this a minute.  Make sure you follow the notation.]
In words: the cgf of a sum of independent random variables
is the sum of the cgf's of the components.
From this it follows that the cumulants of the sum $y$
are the sums of the cumulants of the components $x_1$ and $x_2$.
That is:  If component $x_i$ has $j$th cumulant  $\kappa_{ij}$,
then the $j$th cumulant of $y = x_1 + x_2$ is $ \kappa_{1j} + \kappa_{2j}$.
[If this isn't clear, stare at (\ref{eq:cgf-sum}) and (\ref{eq:cumulant-expansion}).]


We can run through our usual list of examples and see how this works.
In each case, let $x_1$ and $x_2$ be independent random variables with the same distribution.
What is the distribution of $y=x_1 + x_2$?
%
\begin{itemize}
\item {\it Bernoulli.\/}
The cgf of each $x_i$ is
$    k_i(s) = \log \left[ (1-\omega) + \omega e^{s} \right] $.
The cgf for $y$ is the sum of two of these:
$    k_y(s) = 2 \log \left[ (1-\omega) + \omega e^{s} \right] $.
If you look up the cgf for a binomial random variable, you'll see it has this form.

\item {\it Poisson.\/}  The cgf of each $x_i$ is
$  k_i(s) = \omega (e^{s}-1)  $,
so $y$ has cgf
$  k_y(s) = 2\omega (e^{s}-1)  $.
Therefore $y$ inherits the Poisson distribution of the components,
but the intensity parameter changes to $2 \omega$.
This is a useful property; we'll put it to good use when we do option pricing.


\item {\it Normal.\/}
The cgf of each $x_i$ is
$    k_i(s) = \mu s + \sigma^2 s^2/2  $,
so $k_y(s) = 2\mu s + 2\sigma^2 s^2/2$.
This is normal, too, with double the mean and double the variance
(not the standard deviation!).
\end{itemize}

Thinking ahead, sums are a device for introducing nonnormality:
If at least one of the components is nonnormal,
then so is the sum.
[Can you see why that is?]

{\it Mixtures.\/}
A  mixture is a more complicated object,
but also a more interesting one.
A mixture is often defined as a weighted average of probability densities.
Suppose we have a bunch of pdf's $p_i(x)$.
Then a mixture is a random variable whose pdf is a weighted average of pdf's:
\begin{eqnarray*}
    p(x) &=& \sum_i \omega_i p_i(x) ,
\end{eqnarray*}
with weights $\{\omega_i\}$ that are positive and sum to one.
From this we see that $p$ is a legitimate pdf.
[Think about it.]

I regard mixing as a device for generate interesting distributions,
but we could give it a physical interpretation as a two-stage random variable.
First we draw a number $i$ from a distribution with probabilities $\omega_i$.
Given a draw for $i$, we then draw from the distribution of $p_i(x)$.
[Draw two-stage event tree.]

Let's see how this works when we mix two distributions:
with probability $1-\omega$ $x$ has pdf $p_1(x)$
and with probability $\omega$ it has pdf $p_2(x)$.
The overall pdf is therefore
\begin{eqnarray*}
    p (x) &=& (1-\omega) p_1(x) + \omega p_2 (x) .
\end{eqnarray*}
From this it follows that its mgf is also a weighted average:
\begin{eqnarray*}
    h_y(s) &=& (1-\omega) E_1 \left( e^{sx} \right)
                + \omega  E_2 \left( e^{sx} \right)
            \;\;=\;\; (1-\omega) h_1(s) + \omega h_2 (s) ,
\end{eqnarray*}
where $E_j$ means the expectation computed from $p_j(x)$.
The cgf is the log of this.
This gives us the log of a sum, which in some ways is a mess,
but we'll see that it produces some interesting properties.

It's less obvious, perhaps, but this is also a device for introducing nonnormality.
That's true here even if the components are normal.
Some of the most popular models in finance start with mixtures like this.
Most applications of (one of several things called) the Merton option pricing model,
for example, use Poisson mixtures of normals.


\section*{Bottom line}

We have seen how to formalize the idea of random variables,
aka things that are random.
We also have some summary measures that describe it.
The mean and variance describe location and dispersion.
Skewness and excess kurtosis describe the shape of the distribution
and are independent of scale and location.
They are useful in identifying departures from normality,
the first focusing on asymmetry, the second on the tails.
Covariances and correlations describe
relations between random variables.
We'll use them all --- intensively --- from now on.


\section*{Practice problems}

\begin{enumerate}
\item {\it Digital option.\/}
A digital option pays some constant amount --- say, one hundred ---
in some situations, nothing in others.
\begin{enumerate}
\item What set of states do we need to describe this asset's payoffs?
\item What random variable connects the payoffs to the states?
\item Suppose the probability of a positive payoff is $\theta$.
What conditions on $\theta$ guarantee that we have a legitimate probability distribution?
\item What is the mean payoff?
\item What is the variance of the payoff?
The standard deviation?
\item For what value(s) of $\theta$ is the variance largest?  Smallest?
\end{enumerate}
%
\needspace{2\baselineskip}
Answer.
\begin{enumerate}
\item Two states correspond to the two payoffs.  We could use, for example,
$z=0$ for no payoff, $z=1$ for a payoff of one hundred.
\item With this choice of states, the payoff is a random variable $x(z) = 100 z$.
\item Probabilities must be positive and sum to one.
If they sum to one, we have probabilities $(1-\theta, \theta)$,
so $\theta$ must be between zero and one.
\item The mean is $ (1-\theta) \cdot 0 + \theta \cdot 100 = \theta \cdot 100 $.
\item The variance is (I'm using the formula for a Bernoulli random variable)
$ \theta (1-\theta) 100^2$.
The standard deviation is the square root.
\item This is largest when $\theta = 1/2$, smallest (zero) when $\theta$ is zero or one.
\end{enumerate}

\item {\it Sample moments.\/}
Consider the following observations of a random variable $x$:
 $ (2, -1, 4, 3)$.
 Write a Matlab script to answer the following:
\begin{enumerate}
\item What are the first two sample moments?
\item What is the (sample) mean?
\item What is the variance?  The standard deviation?
\end{enumerate}
%
Answer.
Here's a script:
\begin{verbatim}
x = [2,-1,4,3]'
mu1p = sum(x)/4
mu2p = sum(x.^2)/4
mean = mu1p
variance = mu2p - mu1p^2
var_alt = sum((x-mu1p).^2)/4
stddev = sqrt(variance)
\end{verbatim}

\begin{enumerate}
\item The first sample moment is 2 (the average of the observations),
the second is 7.5 (the average of the squared observations).
\item The mean is the first one:  2.
\item The variance is the second moment minus the square of the first:
$7.5 - 2^2 = 3.5 $.
We could also have subtracted the mean from $x$
and computed the average of these squared deviations, which is again 3.5.
[Can you show that both methods give you the same answer?]
The standard deviation is the square root:  $1.87 = 3.5^{1/2} $.
\end{enumerate}

\item {\it Normal random variables.\/}
Suppose $x$ is standard normal:  that is, normal with $\mu = 0$ and $\sigma = 1$.
\begin{enumerate}
\item What is its pdf?
\item What is its cgf?
\item Use the cgf to derive its mean and variance.
\item Now consider $y = \mu + \sigma x$.
What is its cgf?
Use it to derive its mean and variance.
\end{enumerate}
%
\needspace{2\baselineskip}
Answer.
\begin{enumerate}
\item We've seen this one already:
$ p(x) = (2 \pi)^{-1/2} \exp(-x^2/2)$.
\item This one, too:  $ k(s) = s^2/2$.
\item The first derivative gives us the mean.
$k^{(1)}(s) = s$.
The mean is the value at $s=0$, which is zero.
The second derivative gives us the variance:
$k^{(2)}(s) = 1$,
so the variance is one.
\item Apply the definition:
\begin{eqnarray*}
    k_y(s) &=& \log E (e^{sy})
            \;\;=\;\; \log E (e^{s(\mu + \sigma x)})
             \;\;=\;\; \log \left[ e^{s\mu} E ( e^{ s\sigma x}) \right]
             \;\;=\;\; s \mu + k(s \sigma) .
\end{eqnarray*}
That gives us $k_y = s \mu + (s \sigma)^2/2 $.
Its first and second derivatives give us a mean of $\mu$ and a variance of $\sigma^2$.
This bit of Matlab code does the work:
\begin{verbatim}
syms s mu sigma                     % defines these as symbols
cgf_x = s^2/2;
cgf_y = s*mu + subs(cgf_x, s, s*sigma);

kappa1 = subs(diff(cgf_y,s,1),s,0)  % mean
kappa2 = subs(diff(cgf_y,s,2),s,0)  % variance
\end{verbatim}
\end{enumerate}

\item {\it Cumulants and moments.\/}
Moments are clearly defined as expectations of powers of random variables, $E(x^j)$.
But what are cumulants?
That's less clear, but we can connect them to moments using the relation
between the mgf $h(s)$ and cgf $k = \log h(s)$.
\begin{enumerate}
\item What is the $j$th derivative of $h(s)$ evaluated at $s=0$?
\item How are derivatives of $k(s)$ connected to derivatives of $h(s)$?
\item Show that the third cumulant is the same as the third central moment.
\end{enumerate}
%
\needspace{2\baselineskip}
Answer.
\begin{enumerate}
\item This is the $j$th (raw) moment $\mu_j$.
\item Since $k(s) = \log h(s)$, the derivatives of $k$ are connected to those of $h$.
Eg, $k^{(1)}(s) =  h^{(1)}(s)/h(s) $.
That gives us a connection between cumulants (derivatives of $k$) and moments
(derivatives of $h$).
\item We need to work through the derivatives one by one:
\begin{eqnarray*}
    k^{(1)}(s) &=& h^{(1)}(s) / h(s)  \\
    k^{(2)}(s) &=& [h(s)h^{(2)}(s) - h^{(1)}(s)^2]/h(s)^2
            \;\;=\;\; h^{(2)}(s)/h(s) - k^{(1)}(s)^2 \\
    k^{(3)}(s) &=& [h(s)h^{(3)}(s) - h^{(2)}(s)h^{(1)}(s)]/h(s)^2 - 2 k^{(1)}(s) k^{(2)}(s)
\end{eqnarray*}
Now we evaluate at $s=0$.  Since $h(0) = 1$ (why?),
the first three cumulants are
\begin{eqnarray*}
    k^{(1)}(0) &=& h^{(1)}(0) \;\;=\;\; \mu_1^\prime  \\
    k^{(2)}(s) &=& h^{(2)}(0) - h^{(1)}(0)^2 \;\;=\;\; \mu_2^\prime - (\mu_1^\prime)^2 \\
    k^{(3)}(s) &=& h^{(3)}(0) - h^{(2)}(0)h^{(1)}(0)]
                    - 2 k^{(1)}(0) k^{(2)}(0)
               \;\;=\;\; \mu_3^\prime - 3 \mu_2^\prime \mu^\prime_1 + 2(\mu^\prime_1)^3 .
\end{eqnarray*}
It's a little tedious to show, but the final expression is the third
central moment:  $ E (x - \mu_1^\prime)^3$.
To show this, just multiply out the cube and take expectations.
For more along these lines, look up cumulants in Wikipedia.

\end{enumerate}

\item {\it Three-state ``Bernoulli.''\/}
Consider a 3-state distribution
in which we can explore the impact of high-order cumulants.
This is sometimes called a ``categorical distribution.''
Let us say, to be concrete, that the state $z$ takes on the values
 $\{-1, 0, 1\}$ with
 probabilities $\{\omega, 1-2\omega, \omega \}$.
 A random variable $x$ is defined by $x(z) = \delta z$.

\begin{enumerate}
\item Does $x$ have a legitimate probability distribution?
\item What is the moment generating of $x$?  The cumulant generating function?
\item What are the mean and variance of $x$?
\item What are the measures of skewness and excess kurtosis,
$\gamma_1$ and $\gamma_2$?
Under what conditions is $\gamma_2$ large?
\item Is there a value of $\omega$ that reproduces the values
of $\gamma_1$ and $\gamma_2$ of a normal random variable?
\end{enumerate}

\needspace{4\baselineskip}
Answer.
\begin{enumerate}
\item Probabilities are positive (maybe greater than or equal to zero)
and sum to one.
So we're all set if $ 0 < \omega < 1/2$
(or weak inequalities if you prefer).

\item The mgf is
\begin{eqnarray*}
    h(s) &=& \omega (e^{-\delta s} + e^{\delta s}) + (1-2\omega).
\end{eqnarray*}
The cgf is $k(s) = \log h(s)$.

\item The mean and variance are
\begin{eqnarray*}
    \kappa_1  &=& 0 \\
    \kappa_2  &=& \delta^2 2 \omega .
\end{eqnarray*}
One way to find them is from the derivatives of the cgf.

\item Skewness and excess kurtosis are
\begin{eqnarray*}
    \gamma_1  &=& 0 \\
    \gamma_2  &=& 1/(2 \omega) - 3 .
\end{eqnarray*}
The first is clear, because the distribution is symmetric.
The second is a calculation based on cumulants.
Or you could compute the 4th central moment directly,
\begin{eqnarray*}
    \mu_4 &=&  \omega (-\delta)^4 + (1-2\omega) \cdot 0^4 + \omega \delta^4 \;\;=\;\;
        2 \omega \delta^4 ,
\end{eqnarray*}
which implies $\gamma_2 = \mu_4 /(\mu_2)^2 - 3  = 1/2\omega - 3 $.
Evidently $\gamma_2$ is large when $\omega$ is small.

\item The normal has $\gamma_1 = \gamma_2 = 0$.
We get the first automatically and the second if $p = 1/6$.
\end{enumerate}
Matlab code:
\begin{verbatim}
syms s omega delta                  % defines these as symbols
mgf = omega*(exp(-s*delta)+exp(s*delta)) + (1-2*omega);
cgf = log(mgf);

kappa1 = subs(diff(cgf,s,1),s,0)    % mean
kappa2 = subs(diff(cgf,s,2),s,0)    % variance
kappa3 = subs(diff(cgf,s,3),s,0)
kappa4 = subs(diff(cgf,s,4),s,0)

gamma1 = kappa3/kappa2^(3/2)
gamma2 = kappa4/kappa2^2
simplify(gamma2)                    % sometimes this helps
\end{verbatim}


\item {\it Exponential random variables.\/}
We say $x$ is exponential if its pdf is
$ %\begin{eqnarray*}
    p(x) = \lambda e^{-\lambda x}
$ %\end{eqnarray*}
for $x \geq 0$ and $\lambda > 0$.
It's a one-sided distribution and (as we shall see) skewed to the right.
[Graph it, you'll get the idea.]
%
\begin{enumerate}
\item Show that the cgf is
\begin{eqnarray*}
    k(s)  &=& - \log \left( 1 - s/\lambda \right) .
\end{eqnarray*}
\item Use the cgf to compute the variance, skewness, and excess kurtosis
of $x$.  How do they compare to those of normal random variables?
\item Suppose we have a random variable $ y = a + x$
with  a mean of 10 and a standard deviation of 2.
What values of $a$ and $\lambda$ reproduce these values?
\end{enumerate}

\needspace{4\baselineskip}
Answer.
\begin{enumerate}
\item Apply the definition of the mgf:
\begin{eqnarray*}
    h(s) &=& \int_{0}^{\infty} \lambda e^{(s-\lambda)x} dx
            \;\;=\;\; \left. \lambda (s-\lambda)^{-1} e^{(s-\lambda)x} \right|_0^\infty
%            \;\;=\;\; -\lambda / (s-\lambda)
            \;\;=\;\; 1/(1-s/\lambda) .
\end{eqnarray*}
This converges for $s<\lambda$, which includes $s=0$.
The cgf follows from taking the log.
\item The first four cumulants are
$\kappa_1 = 1/\lambda$,
$\kappa_2 = 1/\lambda^2$,
$\kappa_3 = 2/\lambda^3$,
and $\kappa_4 = 6/\lambda^4$.
Skewness and excess kurtosis are therefore
$\gamma_1 = 2$ and $\gamma_2 = 6$.
Both are zero for normal random variables.

\item The mean and standard deviation of $y$ are
$ a + 1/\lambda $ and $1/\lambda$.
We hit the given targets if $\lambda = 1/2 $ and $ a = 8$.
\end{enumerate}
Matlab code:  adapt from earlier problems.

%\item {\it Gamma random variables.\/} ??
% gamma as sum of exponentials
% Poisson mixture of gammas

\item {\it The sum of normals is normal.\/}
The idea here is to use cumulant generating functions (cgfs)
to show that the sum of independent normal random variables is also normal.
It's helpful to break the problem into manageable pieces, like this:
\begin{enumerate}
\item Consider two independent random variables $x_1$ and $x_2$,
    not necessarily normal.
    Show that the cgf of the sum $y = x_1 + x_2$ is the sum of their cgfs:
\begin{eqnarray*}
	k_y(s) &=& k_1(s) + k_2(s) .
\end{eqnarray*}
Hint: Note the form of the pdf and apply the definition of the cgf.

\item Suppose $x_i \sim \mathcal{N}(\kappa_{i1}, \kappa_{i2})$.
[This bit of notation means:   $x_i$ is normally distributed with
mean $\kappa_{i1}$ and variance $\kappa_{i2}$.]
What is $x_i$'s cgf?

\item Use (a) to find the cgf of $y = x_1 + x_2$,
with $(x_1,x_2)$ as described in (b) (namely, normal with given means and variances).
How do you know that $y$ is also normal?  What are its mean and variance?

\item Extend this result to $y = a x_1 + b x_2$ for any real numbers $(a,b)$.
\end{enumerate}
%
\needspace{3\baselineskip}
Answer.
\begin{enumerate}
\item Recall that if $x_1$ and $x_2$ are independent, their pdf factors:
$ p_{12}(x_1,x_2) = p_1(x_1) p_2(x_2) $.
That means the mgf of $x_1+x_2$ is the product of their individual mgf's:
$ h_y(s) = h_1(s) h_2(s) $.
We take the log to get the cgf's:
$ k_y(s) = \log h_y(s) = \log h_1(s) + \log h_2(s) = k_1(s) + k_2(s) $.

\item $  k_i(s) = s \kappa_{1i} + s^2 \kappa_{2i}/2 $.
(If this isn't burned into your memory already, please burn it in now.)
\item Sum the cgf's:
\begin{eqnarray*}
    k_y(s) &=& k_1(s) + k_2(s) \\
            &=& (s \kappa_{11} + s^2 \kappa_{21}/2) + (s\kappa_{12} + s^2 \kappa_{22}/2) \\
            &=& s (\kappa_{11} + \kappa_{12} )
                + s^2 (\kappa_{21} + \kappa_{22})/2 .
\end{eqnarray*}
It's normal because its mgf has the form of a normal random variable:
quadratic in $s$.
In fact, we can pick the mean and variance right out of the formula.
\item Still normal, but with a change in mean and variance:
\begin{eqnarray*}
    k_y(s) &=& s (a \kappa_{11} + b \kappa_{12} )
                + s^2 (a^2 \kappa_{21} + b^2 \kappa_{22})/2  .
\end{eqnarray*}
\end{enumerate}


% do variant in hw -- change variance
\item {\it Normal mixtures.\/}
We'll use a Bernoulli mixture of normals to produce
random variables that are decidedly not normal.
Consider $x \sim \mathcal{N}(0,1)$
with probability $1-\omega$
and $x \sim \mathcal{N}(\theta,1)$ with probability $\omega$.
The idea is that $x$ is standard normal most of the time (probability $1-\omega$),
but once in a while (probability $\omega$, with $\omega$ small)
we get a draw from a normal with a different mean ($\theta$).
What does this do to the distribution?
Let's see.
%
\begin{enumerate}
\item What is the pdf?  What does it look like?
\item What is $x$'s cgf?
\item What are the first three cumulants?
\item How does this differ from a normal random variable?
\end{enumerate}
Suggestion:  Let Matlab do most of the work.

\needspace{4\baselineskip}
Answer.
\begin{enumerate}
\item The pdf is a weighted average of the two normal components:
\begin{eqnarray*}
        p(x) &=& (1-\omega) (2 \pi)^{-1/2} \exp(-x^2/2) +
            \omega \; (2 \pi)^{-1/2} \exp[-(x-\theta)^2/2 ].
\end{eqnarray*}
It's a weighted average of two normals.
\item The cgf is
\begin{eqnarray*}
    k(s) &=&  \log \left[ (1-\omega) e^{s^2/2}
                + \omega  e^{\theta s + s^2/2} \right].
\end{eqnarray*}
No need to derive this, it follows immediately from things we've done already:
Bernoulli mixtures and normal mgf's.
\item Matlab gives us
\begin{eqnarray*}
    \kappa_1 &=& \omega \theta \\
    \kappa_2 &=& \theta^2 \omega (1-\omega) + 1 \\
    \kappa_3 &=& \theta^3 \omega (1-\omega) (1-2\omega) .
\end{eqnarray*}
Here's the code:
\begin{verbatim}
syms s theta omega                  % defines these as symbols
mgf = (1-omega)*exp(s^2/2) + omega*exp(theta*s + s^2/2);
cgf = log(mgf);
kappa1 = subs(diff(cgf,s,1),s,0)    % mean
kappa2 = subs(diff(cgf,s,2),s,0)    % variance
kappa3 = subs(diff(cgf,s,3),s,0)
factor(kappa3)  % sometimes cleans up the expression
\end{verbatim}
\item It's obviously not normal, but the key is the third cumulant $\kappa_3$,
which is zero for normals
but takes the sign of $\theta$ here.
%If $\theta<0$ we have negative skewness.
The rest is similar to the Bernoulli on its own.
If you'd like a picture, try this:
\begin{verbatim}
x = [-4:0.1:4]';

p1 = exp(-x.^2/2)./sqrt(2*pi);
mu = -2; sigma = 1;
p2 = exp(-(x-mu).^2/(2*sigma^2))./sqrt(2*pi*sigma^2);
omega = 0.2;
pmix = (1-omega)*p1 + omega*p2;

plot(x,p1,'b')
hold on
plot(x,pmix,'m')
text(0.4, 0.38, 'blue=std normal, magenta=mixture')
\end{verbatim}
\end{enumerate}

%\item {\it Poisson mixtures.\/}

\item {\it Sample covariances and correlations.\/}
We continue an earlier problem, where $ x= (2,-1,4,3)$,
and add $y = (10,-5, 3, 0)$.
\begin{enumerate}
\item What is the (sample) variance of $y$?
\item What is the covariance of $x$ and $y$?
\item What is the correlation of $x$ and $y$?
\end{enumerate}

\needspace{4\baselineskip}
Answer.
\begin{enumerate}
\item The sample variance is 29.5.
\item The sample covariance is 5.25.
\item The correlation is
\begin{eqnarray*}
    \mbox{Corr}(x,y) &=&
            \frac {\mbox{Cov}(x,x)} {\mbox{Var}(x)^{1/2} \mbox{Var}(y)^{1/2} }
            \;\;=\;\; \frac{ 5.25} {3.5^{1/2} 29.5^{1/2}}
            \;\;=\;\; 0.5167 .
\end{eqnarray*}
We noted earlier that correlations are independent of scale.
You can verify that by replacing $y$ with $ 3y+17$ and redoing the calculations.
Sample Matlab code:
\begin{verbatim}
x = [2,-1,4,3]'
y = [10, -5, 3, 0]'
xbar = mean(x)
ybar = mean(y)
varx = mean((x-xbar).^2)
vary = mean((y-ybar).^2)
covxy = mean((x-xbar).*(y-ybar))
corrxy = covxy/sqrt(varx*vary)
\end{verbatim}
\end{enumerate}

\end{enumerate}


\section*{More}

This is basic probability theory, which you can find in lots of places.
You should think seriously about taking any course with a similar title.
One thing to keep in mind:
the level of mathematical sophistication ranges from low to extremely high.
What we've done is between low and medium, but that's not a bad place to be.

Wikipedia is very good for properties of particular distributions.
Search for (say) ``Poisson distribution'' and you'll find lots of what we've done here,
and more.
Wikipedia's also ok on the underlying mathematics, but you're never sure what
level you'll get:  too high, too low, or just right.

\input{../LaTeX/footer.tex}

\end{document}

\subsection*{If you'd like to know more}

The notes are enough -- maybe more than enough.
What this section is about...



Gen functions:  Wilf...  (just for fun)

Courses:  probability, measure theory...

The general theory of what we've done here is referred to as ``probability theory.''
It's never a bad idea to take a course with a title similar to that.
The concepts are developed further in ``measure theory,''
also a good idea if you have time for some moderately advanced math courses.

For properties of particular distributions, I find Wikipedia helpful.
Search for (say) ``Poisson distribution'' and you'll find everything we've done here
and more.
Wikipedia's also ok on the underlying mathematics, but you're never sure what
level you'll get:  too high, too low, or just right.

Software:

Numerical:  Octave is an open source version of Matlab, identical on the basics.
See also GUI Octave.
SciLab is said to be similar.

Statistics:  R is an open source program .  Also RStudio.

Symbolic math:  Maxima, Mathematica.


Measure theory...


\end{document}
